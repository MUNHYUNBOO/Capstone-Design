{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aada3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyedflib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 고정 설정\n",
    "sampling_rate = 256\n",
    "segment_length = 30 * sampling_rate\n",
    "label_map = {\n",
    "    'Sleep stage W': 0,\n",
    "    'Sleep stage N1': 1,\n",
    "    'Sleep stage N2': 1,\n",
    "    'Sleep stage N3': 2,\n",
    "    'Sleep stage R': 3\n",
    "}\n",
    "root_dir = \"/home/mhb0917/캡스톤디자인/sleep/recordings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce987686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_sn(sn_list):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "\n",
    "    for sn in sn_list:\n",
    "        base = f\"SN{sn:03d}\"\n",
    "        ecg_path = os.path.join(root_dir, f\"{base}.edf\")\n",
    "        label_path = os.path.join(root_dir, f\"{base}_sleepscoring.edf\")\n",
    "\n",
    "        if not os.path.exists(ecg_path) or not os.path.exists(label_path):\n",
    "            print(f\"❌ 파일 없음: {base}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ecg_reader = pyedflib.EdfReader(ecg_path)\n",
    "            ecg_signal = ecg_reader.readSignal(7)\n",
    "        finally:\n",
    "            ecg_reader._close()\n",
    "\n",
    "        try:\n",
    "            label_reader = pyedflib.EdfReader(label_path)\n",
    "            annotations = label_reader.readAnnotations()\n",
    "        finally:\n",
    "            label_reader._close()\n",
    "\n",
    "        onsets, durations, labels = annotations\n",
    "        label_seq = []\n",
    "        for i in range(len(labels)):\n",
    "            label_str = labels[i].decode() if isinstance(labels[i], bytes) else labels[i]\n",
    "            if label_str in label_map:\n",
    "                label_index = label_map[label_str]\n",
    "                num_segments = int(durations[i] // 30)\n",
    "                label_seq.extend([label_index] * num_segments)\n",
    "\n",
    "        max_segments = len(ecg_signal) // segment_length\n",
    "        num_segments = min(len(label_seq), max_segments)\n",
    "\n",
    "        for i in range(num_segments):\n",
    "            start = i * segment_length\n",
    "            end = start + segment_length\n",
    "            segment = ecg_signal[start:end]\n",
    "\n",
    "            # ✅ ECG 정규화 (Z-score 방식)\n",
    "            segment = (segment - np.mean(segment)) / (np.std(segment) + 1e-6)\n",
    "\n",
    "            all_segments.append(segment)\n",
    "            all_labels.append(label_seq[i])\n",
    "\n",
    "    X = torch.tensor(np.array(all_segments), dtype=torch.float32).unsqueeze(1)\n",
    "    y = torch.tensor(all_labels, dtype=torch.long)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ec1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_balanced_data_from_sn(sn_list):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "\n",
    "    for sn in sn_list:\n",
    "        try:\n",
    "            X, y = load_data_from_sn([sn])  # SN 하나만 불러옴\n",
    "        except Exception as e:\n",
    "            print(f\"❌ SN{sn:03d} 불러오기 실패: {e}\")\n",
    "            continue\n",
    "\n",
    "        if len(y) == 0:\n",
    "            print(f\"⚠️ SN{sn:03d}: 라벨 없음 → 건너뜀\")\n",
    "            continue\n",
    "\n",
    "        y_np = y.numpy()\n",
    "        X_np = X.numpy()\n",
    "\n",
    "        label_counts = Counter(y_np)\n",
    "        if len(label_counts) < 2:\n",
    "            print(f\"⚠️ SN{sn:03d}: 라벨 다양성 부족 → 건너뜀\")\n",
    "            continue\n",
    "\n",
    "        min_count = min(label_counts.values())\n",
    "        indices = []\n",
    "\n",
    "        for label in sorted(label_counts.keys()):\n",
    "            label_indices = np.where(y_np == label)[0]\n",
    "            if len(label_indices) < min_count:\n",
    "                continue\n",
    "            sampled = np.random.choice(label_indices, min_count, replace=False)\n",
    "            indices.extend(sampled)\n",
    "\n",
    "        if not indices:\n",
    "            print(f\"⚠️ SN{sn:03d}: 유효한 샘플 없음 → 건너뜀\")\n",
    "            continue\n",
    "\n",
    "        np.random.shuffle(indices)\n",
    "        all_segments.append(X_np[indices])\n",
    "        all_labels.append(y_np[indices])\n",
    "\n",
    "        print(f\"✅ SN{sn:03d} 처리 완료: 클래스당 {min_count}개\")\n",
    "\n",
    "    if not all_segments:\n",
    "        raise ValueError(\"⚠️ 유효한 데이터가 없습니다. SN 목록을 다시 확인하세요.\")\n",
    "\n",
    "    X_all = torch.tensor(np.concatenate(all_segments), dtype=torch.float32)\n",
    "    y_all = torch.tensor(np.concatenate(all_labels), dtype=torch.long)\n",
    "    return X_all, y_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55fee5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SN001 처리 완료: 클래스당 23개\n",
      "✅ SN002 처리 완료: 클래스당 82개\n",
      "✅ SN003 처리 완료: 클래스당 150개\n",
      "✅ SN004 처리 완료: 클래스당 64개\n",
      "✅ SN005 처리 완료: 클래스당 127개\n",
      "✅ SN006 처리 완료: 클래스당 65개\n",
      "✅ SN007 처리 완료: 클래스당 67개\n",
      "✅ SN008 처리 완료: 클래스당 39개\n",
      "✅ SN009 처리 완료: 클래스당 91개\n",
      "✅ SN010 처리 완료: 클래스당 101개\n",
      "✅ SN011 처리 완료: 클래스당 37개\n",
      "✅ SN012 처리 완료: 클래스당 184개\n",
      "✅ SN013 처리 완료: 클래스당 65개\n",
      "❌ 파일 없음: SN014\n",
      "⚠️ SN014: 라벨 없음 → 건너뜀\n",
      "✅ SN015 처리 완료: 클래스당 102개\n",
      "✅ SN016 처리 완료: 클래스당 24개\n",
      "✅ SN017 처리 완료: 클래스당 96개\n",
      "✅ SN018 처리 완료: 클래스당 19개\n",
      "✅ SN019 처리 완료: 클래스당 72개\n",
      "✅ SN020 처리 완료: 클래스당 67개\n",
      "✅ SN021 처리 완료: 클래스당 156개\n",
      "✅ SN022 처리 완료: 클래스당 115개\n",
      "✅ SN023 처리 완료: 클래스당 74개\n",
      "✅ SN024 처리 완료: 클래스당 70개\n",
      "✅ SN025 처리 완료: 클래스당 106개\n",
      "✅ SN026 처리 완료: 클래스당 127개\n",
      "✅ SN027 처리 완료: 클래스당 19개\n",
      "✅ SN028 처리 완료: 클래스당 136개\n",
      "✅ SN029 처리 완료: 클래스당 59개\n",
      "✅ SN030 처리 완료: 클래스당 108개\n",
      "✅ SN031 처리 완료: 클래스당 74개\n",
      "✅ SN032 처리 완료: 클래스당 133개\n",
      "✅ SN033 처리 완료: 클래스당 47개\n",
      "✅ SN034 처리 완료: 클래스당 75개\n",
      "✅ SN035 처리 완료: 클래스당 108개\n",
      "✅ SN036 처리 완료: 클래스당 87개\n",
      "✅ SN037 처리 완료: 클래스당 65개\n",
      "✅ SN038 처리 완료: 클래스당 61개\n",
      "✅ SN039 처리 완료: 클래스당 108개\n",
      "✅ SN040 처리 완료: 클래스당 62개\n",
      "✅ SN041 처리 완료: 클래스당 130개\n",
      "✅ SN042 처리 완료: 클래스당 148개\n",
      "✅ SN043 처리 완료: 클래스당 56개\n",
      "✅ SN044 처리 완료: 클래스당 55개\n",
      "✅ SN045 처리 완료: 클래스당 129개\n",
      "✅ SN046 처리 완료: 클래스당 49개\n",
      "✅ SN047 처리 완료: 클래스당 125개\n",
      "✅ SN048 처리 완료: 클래스당 146개\n",
      "✅ SN049 처리 완료: 클래스당 41개\n",
      "✅ SN050 처리 완료: 클래스당 25개\n",
      "✅ SN051 처리 완료: 클래스당 84개\n",
      "✅ SN052 처리 완료: 클래스당 59개\n",
      "✅ SN053 처리 완료: 클래스당 163개\n",
      "✅ SN054 처리 완료: 클래스당 28개\n",
      "✅ SN055 처리 완료: 클래스당 115개\n",
      "✅ SN056 처리 완료: 클래스당 64개\n",
      "✅ SN057 처리 완료: 클래스당 7개\n",
      "✅ SN058 처리 완료: 클래스당 103개\n",
      "✅ SN059 처리 완료: 클래스당 138개\n",
      "✅ SN060 처리 완료: 클래스당 46개\n",
      "✅ SN061 처리 완료: 클래스당 150개\n",
      "✅ SN062 처리 완료: 클래스당 5개\n",
      "✅ SN063 처리 완료: 클래스당 139개\n",
      "❌ 파일 없음: SN064\n",
      "⚠️ SN064: 라벨 없음 → 건너뜀\n",
      "✅ SN065 처리 완료: 클래스당 65개\n",
      "✅ SN066 처리 완료: 클래스당 88개\n",
      "✅ SN067 처리 완료: 클래스당 123개\n",
      "✅ SN068 처리 완료: 클래스당 116개\n",
      "✅ SN069 처리 완료: 클래스당 69개\n",
      "✅ SN070 처리 완료: 클래스당 35개\n",
      "✅ SN071 처리 완료: 클래스당 16개\n",
      "✅ SN072 처리 완료: 클래스당 115개\n",
      "✅ SN073 처리 완료: 클래스당 39개\n",
      "✅ SN074 처리 완료: 클래스당 37개\n",
      "✅ SN075 처리 완료: 클래스당 186개\n",
      "✅ SN076 처리 완료: 클래스당 56개\n",
      "✅ SN077 처리 완료: 클래스당 85개\n",
      "✅ SN078 처리 완료: 클래스당 55개\n",
      "✅ SN079 처리 완료: 클래스당 61개\n",
      "✅ SN080 처리 완료: 클래스당 28개\n",
      "✅ SN081 처리 완료: 클래스당 2개\n",
      "✅ SN082 처리 완료: 클래스당 37개\n",
      "✅ SN083 처리 완료: 클래스당 136개\n",
      "✅ SN084 처리 완료: 클래스당 27개\n",
      "✅ SN085 처리 완료: 클래스당 111개\n",
      "✅ SN086 처리 완료: 클래스당 110개\n",
      "✅ SN087 처리 완료: 클래스당 34개\n",
      "✅ SN088 처리 완료: 클래스당 125개\n",
      "✅ SN089 처리 완료: 클래스당 85개\n",
      "✅ SN090 처리 완료: 클래스당 76개\n",
      "✅ SN091 처리 완료: 클래스당 40개\n",
      "✅ SN092 처리 완료: 클래스당 93개\n",
      "✅ SN093 처리 완료: 클래스당 71개\n",
      "✅ SN094 처리 완료: 클래스당 79개\n",
      "✅ SN095 처리 완료: 클래스당 50개\n",
      "✅ SN096 처리 완료: 클래스당 55개\n",
      "✅ SN097 처리 완료: 클래스당 46개\n",
      "✅ SN098 처리 완료: 클래스당 80개\n",
      "✅ SN099 처리 완료: 클래스당 56개\n",
      "✅ SN100 처리 완료: 클래스당 81개\n",
      "✅ SN101 처리 완료: 클래스당 120개\n",
      "✅ SN102 처리 완료: 클래스당 36개\n",
      "✅ SN103 처리 완료: 클래스당 70개\n",
      "✅ SN104 처리 완료: 클래스당 104개\n",
      "✅ SN105 처리 완료: 클래스당 34개\n",
      "✅ SN106 처리 완료: 클래스당 53개\n",
      "✅ SN107 처리 완료: 클래스당 77개\n",
      "✅ SN108 처리 완료: 클래스당 50개\n",
      "✅ SN109 처리 완료: 클래스당 48개\n",
      "✅ SN110 처리 완료: 클래스당 96개\n",
      "✅ SN111 처리 완료: 클래스당 135개\n",
      "✅ SN112 처리 완료: 클래스당 83개\n",
      "✅ SN113 처리 완료: 클래스당 85개\n",
      "✅ SN114 처리 완료: 클래스당 73개\n",
      "✅ SN115 처리 완료: 클래스당 98개\n",
      "✅ SN116 처리 완료: 클래스당 43개\n",
      "✅ SN117 처리 완료: 클래스당 108개\n",
      "✅ SN118 처리 완료: 클래스당 114개\n",
      "✅ SN119 처리 완료: 클래스당 89개\n",
      "✅ SN120 처리 완료: 클래스당 29개\n",
      "✅ SN121 처리 완료: 클래스당 61개\n",
      "✅ SN122 처리 완료: 클래스당 66개\n",
      "✅ SN123 처리 완료: 클래스당 98개\n",
      "❌ 파일 없음: SN135\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_balanced_data_from_sn(list(range(1, 94)))  # SN001 ~ SN093\n",
    "X_val, y_val = load_balanced_data_from_sn(list(range(94, 124)))   # SN094 ~ SN123\n",
    "X_test, y_test = load_data_from_sn(list(range(124, 154)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90d79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 🔹 모델 정의\n",
    "class ECGSleepCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ECGSleepCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(64 * 960, 256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 🔹 데이터로더 설정\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
    "\n",
    "# 🔹 학습 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ECGSleepCNN(num_classes=4).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 🔹 학습 루프 \n",
    "for epoch in range(50):  # 최대 50 epoch\n",
    "    model.train()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        total += yb.size(0)\n",
    "        correct += (pred == yb).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # 🔹 검증 단계\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            val_total += yb.size(0)\n",
    "            val_correct += (pred == yb).sum().item()\n",
    "\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"📘 Epoch {epoch+1} | Train Loss: {loss_sum:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b290e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test), 32):\n",
    "        xb = X_test[i:i+32].to(device)\n",
    "        yb = y_test[i:i+32].to(device)\n",
    "        outputs = model(xb)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['W', 'N12', 'N3', 'R']\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"📄 Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
