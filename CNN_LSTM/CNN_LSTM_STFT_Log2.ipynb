{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pyedflib\n",
    "from scipy.stats import zscore\n",
    "from scipy.signal import stft\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ⚙️ 설정\n",
    "root_dir = \"/home/mhb0917/캡스톤디자인/sleep/recordings\"\n",
    "seq_len = 10\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "label_map = {\n",
    "    'Sleep stage W': 0,\n",
    "    'Sleep stage N1': 1,\n",
    "    'Sleep stage N2': 1,\n",
    "    'Sleep stage N3': 2,\n",
    "    'Sleep stage R': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5e7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ 데이터 로드\n",
    "def load_ecg_and_labels(sn_id, root_dir):\n",
    "    base = f\"SN{int(sn_id):03d}\"\n",
    "    ecg_path = os.path.join(root_dir, f\"{base}.edf\")\n",
    "    label_path = os.path.join(root_dir, f\"{base}_sleepscoring.edf\")\n",
    "    if not os.path.exists(ecg_path) or not os.path.exists(label_path):\n",
    "        print(f\"❌ 파일 없음: {base}\")\n",
    "        return [], []\n",
    "\n",
    "    with pyedflib.EdfReader(ecg_path) as ecg_reader:\n",
    "        ecg_signal = ecg_reader.readSignal(7)\n",
    "\n",
    "    with pyedflib.EdfReader(label_path) as label_reader:\n",
    "        onsets, durations, labels = label_reader.readAnnotations()\n",
    "\n",
    "    segments, segment_labels = [], []\n",
    "    for onset, duration, label in zip(onsets, durations, labels):\n",
    "        label_str = label.decode() if isinstance(label, bytes) else label\n",
    "        if label_str not in label_map:\n",
    "            continue\n",
    "        start = int(onset * 256)\n",
    "        num_segments = int(duration // 30)\n",
    "        for i in range(num_segments):\n",
    "            seg_start = start + i * 7680\n",
    "            seg_end = seg_start + 7680\n",
    "            if seg_end > len(ecg_signal):\n",
    "                break\n",
    "            segment = zscore(ecg_signal[seg_start:seg_end])\n",
    "            if np.isnan(segment).any():\n",
    "                continue\n",
    "            segments.append(segment.astype(np.float32))\n",
    "            segment_labels.append(label_map[label_str])\n",
    "    return segments, segment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ STFT 함수\n",
    "def compute_stft(segment, fs=256, nperseg=256, noverlap=128):\n",
    "    f, t, Zxx = stft(segment, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    # 로그 스케일 변환\n",
    "    magnitude = 10 * np.log10(np.abs(Zxx) + 1e-8)\n",
    "    return magnitude.T  # (time, freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82099291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ STFT + 시퀀스 생성\n",
    "def create_sequences_stft(segments, labels, seq_len=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(segments) - seq_len + 1):\n",
    "        stft_seqs = []\n",
    "        for seg in segments[i:i+seq_len]:\n",
    "            stft_feat = compute_stft(seg)  # (time, freq)\n",
    "            stft_seqs.append(stft_feat)  # (time, freq)\n",
    "        X.append(np.stack(stft_seqs))  # (seq_len, time, freq)\n",
    "        y.append(labels[i + seq_len - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def merge_data(data_dict):\n",
    "    X_all, y_all = [], []\n",
    "    for X, y in data_dict.values():\n",
    "        if len(X) == 0:\n",
    "            continue\n",
    "        X_all.append(X)\n",
    "        y_all.append(y)\n",
    "    if len(X_all) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    return np.concatenate(X_all), np.concatenate(y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f92522da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Dataset\n",
    "class SleepDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ 모델 정의\n",
    "class CNN_STFT_LSTM(nn.Module):\n",
    "    def __init__(self, time_bins, freq_bins, cnn_out=64, lstm_hidden=128, num_classes=4):\n",
    "        super(CNN_STFT_LSTM, self).__init__()\n",
    "        \n",
    "        # 1️⃣ CNN Feature Extractor (3단계 CNN)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, cnn_out, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "            nn.BatchNorm2d(cnn_out),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # (batch*seq, channels, 1, 1)\n",
    "        )\n",
    "        \n",
    "        # 2️⃣ LSTM for Sequence Modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_out,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 3️⃣ Fully Connected Output Layer\n",
    "        self.fc = nn.Linear(lstm_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, time_bins, freq_bins)\n",
    "        batch_size, seq_len, time_bins, freq_bins = x.shape\n",
    "        \n",
    "        # CNN 입력을 위해 채널 차원 추가 (1 채널)\n",
    "        x = x.view(batch_size * seq_len, 1, time_bins, freq_bins)  # (batch*seq, 1, time, freq)\n",
    "        \n",
    "        # CNN 처리\n",
    "        x = self.cnn(x)  # (batch*seq, cnn_out, 1, 1)\n",
    "        x = x.view(batch_size, seq_len, -1)  # (batch, seq_len, cnn_out)\n",
    "        \n",
    "        # LSTM 처리\n",
    "        out, _ = self.lstm(x)  # (batch, seq_len, lstm_hidden)\n",
    "        \n",
    "        # 마지막 시퀀스의 출력만 사용\n",
    "        out = out[:, -1, :]  # (batch, lstm_hidden)\n",
    "        \n",
    "        # 최종 분류\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ 학습 및 평가 함수\n",
    "def train_model(model, train_loader, val_loader, epochs=50, lr=0.0001, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total if total > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {avg_train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, loader, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = outputs.argmax(1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, labels=[0,1,2,3], target_names=[\"W\", \"N1/2\", \"N3\", \"R\"], zero_division=0))\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0,1,2,3])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"W\", \"N1/2\", \"N3\", \"R\"])\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bff446",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m val_data, test_data \u001b[38;5;241m=\u001b[39m {}, {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m155\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     segments, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_ecg_and_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m segments:\n\u001b[1;32m      7\u001b[0m         X_seq, y_seq \u001b[38;5;241m=\u001b[39m create_sequences_stft(segments, labels, seq_len)\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mload_ecg_and_labels\u001b[0;34m(sn_id, root_dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyedflib\u001b[38;5;241m.\u001b[39mEdfReader(ecg_path) \u001b[38;5;28;01mas\u001b[39;00m ecg_reader:\n\u001b[0;32m---> 11\u001b[0m     ecg_signal \u001b[38;5;241m=\u001b[39m \u001b[43mecg_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadSignal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyedflib\u001b[38;5;241m.\u001b[39mEdfReader(label_path) \u001b[38;5;28;01mas\u001b[39;00m label_reader:\n\u001b[1;32m     14\u001b[0m     onsets, durations, labels \u001b[38;5;241m=\u001b[39m label_reader\u001b[38;5;241m.\u001b[39mreadAnnotations()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyedflib/edfreader.py:830\u001b[0m, in \u001b[0;36mEdfReader.readSignal\u001b[0;34m(self, chn, start, n, digital)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_digital_signal(chn, start, n, x)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 830\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadsignal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5️⃣ 데이터 처리 및 학습\n",
    "train_X, train_y = [], []\n",
    "val_data, test_data = {}, {}\n",
    "\n",
    "for i in range(1, 155):\n",
    "    segments, labels = load_ecg_and_labels(i, root_dir)\n",
    "    if segments is not None and len(segments) > 0:\n",
    "        X_seq, y_seq = create_sequences_stft(segments, labels, seq_len)\n",
    "        if len(X_seq) == 0:\n",
    "            continue\n",
    "        if 1 <= i <= 93:\n",
    "            train_X.append(X_seq)\n",
    "            train_y.append(y_seq)\n",
    "        elif 94 <= i <= 123:\n",
    "            val_data[f\"SN{i:03d}\"] = (X_seq, y_seq)\n",
    "        else:\n",
    "            test_data[f\"SN{i:03d}\"] = (X_seq, y_seq)\n",
    "\n",
    "train_X = np.concatenate(train_X)\n",
    "train_y = np.concatenate(train_y)\n",
    "\n",
    "val_X, val_y = [], []\n",
    "for X, y in val_data.values():\n",
    "    val_X.append(X)\n",
    "    val_y.append(y)\n",
    "val_X = np.concatenate(val_X)\n",
    "val_y = np.concatenate(val_y)\n",
    "\n",
    "time_bins, freq_bins = train_X.shape[2], train_X.shape[3]\n",
    "print(f\"✅ Train shape: {train_X.shape}, time={time_bins}, freq={freq_bins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(SleepDataset(train_X, train_y), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(SleepDataset(val_X, val_y), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = CNN_STFT_LSTM(time_bins, freq_bins)\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=epochs, lr=learning_rate, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 그래프 출력\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e849f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9️⃣ 전체 평가\n",
    "X_val_all, y_val_all = merge_data(val_data)\n",
    "if len(X_val_all) > 0:\n",
    "    print(\"\\n📊 Validation (전체) 평가\")\n",
    "    evaluate_model(model, DataLoader(SleepDataset(X_val_all, y_val_all), batch_size=batch_size, shuffle=False))\n",
    "\n",
    "X_test_all, y_test_all = merge_data(test_data)\n",
    "if len(X_test_all) > 0:\n",
    "    print(\"\\n📊 Test (전체) 평가\")\n",
    "    evaluate_model(model, DataLoader(SleepDataset(X_test_all, y_test_all), batch_size=batch_size, shuffle=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
